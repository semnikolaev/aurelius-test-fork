version: '3'

services:
  dev:
    build: .
    environment:
      FLINK_PROPERTIES: |
        jobmanager.rpc.address: localhost
        taskmanager.numberOfTaskSlots: 2
    command: 'taskmanager --cap-add=SYS_PTRACE --security-opt seccomp=unconfined'
    cap_add:
      - SYS_PTRACE
    volumes:
      - ..:/workspace
    ports:
      - "127.0.0.1:5601:5601"
      - "127.0.0.1:8081:8081"
      - "127.0.0.1:8082:8082"
      - "127.0.0.1:8180:8180"
      - "127.0.0.1:9200:9200"
      - "127.0.0.1:3002:3002"
      - "127.0.0.1:21000:21000"
    working_dir: /workspace

  atlas:
    image: sburn/apache-atlas:2.2.0
    depends_on:
      - kafka-broker
    volumes:
      - atlas-data:/opt/apache-atlas-2.2.0/data
    network_mode: service:dev
    restart: unless-stopped
    configs:
      - source: atlas-application
        target: /opt/apache-atlas-2.2.0/conf/atlas-application.properties
      - source: atlas-simple-authz-policy
        target: /opt/apache-atlas-2.2.0/conf/atlas-simple-authz-policy.json
      - source: atlas-keycloak-conf
        target: /opt/apache-atlas-2.2.0/conf/keycloak-conf.json

  jobmanager:
    image: flink:1.17.0
    environment:
      FLINK_PROPERTIES: |
        jobmanager.rpc.address: localhost
        taskmanager.numberOfTaskSlots: 2
    command: jobmanager
    volumes:
      - checkpoints:/tmp/flink-checkpoints-directory
    network_mode: service:dev
    restart: unless-stopped

  kafka-broker:
    image: confluentinc/cp-kafka:7.6.1
    environment:
      CLUSTER_ID: 'ZDhiOWIzN2FmNmE0NWNlNW'
      KAFKA_ADVERTISED_LISTENERS: 'INTERNAL://localhost:9092'
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true'
      KAFKA_BROKER_ID: 1
      KAFKA_CONTROLLER_LISTENER_NAMES: 'CONTROLLER'
      KAFKA_CONTROLLER_QUORUM_VOTERS: '1@localhost:9093'
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_INTER_BROKER_LISTENER_NAME: 'INTERNAL'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: 'CONTROLLER:PLAINTEXT,INTERNAL:PLAINTEXT'
      KAFKA_LISTENERS: 'INTERNAL://localhost:9092,CONTROLLER://localhost:9093'
      KAFKA_NODE_ID: 1
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_PROCESS_ROLES: 'broker,controller'
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1

    volumes:
      - kafka-data:/var/lib/kafka/data
    network_mode: service:dev
    restart: unless-stopped

  kafka-connect:
    build:
      context: ..
      dockerfile: ./services/kafka-connect/Dockerfile
      args:
        CONNECT_ELASTICSEARCH_VERSION: '14.0.11'
        CONNECT_VERSION: '7.3.2'
    depends_on:
      - kafka-broker
    volumes:
      - certs:/home/appuser/certs
    environment:
      CONNECT_BOOTSTRAP_SERVERS: 'localhost:9092'
      CONNECT_CONFIG_STORAGE_CLEANUP_POLICY: 'compact'
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_CONFIG_STORAGE_TOPIC: 'kafka-connect-configs'
      CONNECT_GROUP_ID: 'kafka-connect-group'
      CONNECT_KEY_CONVERTER: 'org.apache.kafka.connect.storage.StringConverter'
      CONNECT_KEY_CONVERTER_SCHEMAS_ENABLE: 'false'
      CONNECT_LOG4J_ROOT_LOGLEVEL: 'INFO'
      CONNECT_OFFSET_STORAGE_CLEANUP_POLICY: 'compact'
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_OFFSET_STORAGE_TOPIC: 'kafka-connect-offsets'
      CONNECT_REST_ADVERTISED_HOST_NAME: 'localhost'
      CONNECT_REST_PORT: 8083
      CONNECT_STATUS_STORAGE_CLEANUP_POLICY: 'compact'
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_STATUS_STORAGE_TOPIC: 'kafka-connect-status'
      CONNECT_VALUE_CONVERTER: 'org.apache.kafka.connect.json.JsonConverter'
      CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: 'false'
    network_mode: service:dev

  kafka-ui:
    image: provectuslabs/kafka-ui:v0.7.2
    depends_on:
      - kafka-broker
    environment:
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: 'localhost:9092'
      KAFKA_CLUSTERS_0_NAME: 'local'
      SERVER_PORT: 8082
    network_mode: service:dev
    restart: unless-stopped

  keycloak:
    image: quay.io/keycloak/keycloak:16.1.1
    environment:
      KEYCLOAK_IMPORT: '/tmp/keycloak-config/atlas-dev.json'
      KEYCLOAK_PASSWORD: 'admin'
      KEYCLOAK_PORT: 8180
      KEYCLOAK_REALM_NAME: 'atlas-dev'
      KEYCLOAK_USER: 'admin'
    volumes:
      - keycloak-data:/opt/jboss/keycloak/standalone/data
    configs:
      - source: keycloak-config
        target: /opt/jboss/keycloak/standalone/configuration/standalone.xml
      - source: keycloak-realm
        target: /tmp/keycloak-config/atlas-dev.json
    network_mode: service:dev
    restart: unless-stopped
    command: ['-b 0.0.0.0', '-bmanagement 0.0.0.0', '-Djboss.http.port=8180']

  #################
  # ELASTIC STACK #
  #################
  setup:
    image: docker.elastic.co/elasticsearch/elasticsearch:${ELASTIC_STACK_VERSION}
    user: "0"
    network_mode: service:dev
    volumes:
      - certs:/usr/share/elasticsearch/config/certs
    command: >
      bash -c '
        if [ x${ELASTIC_PASSWORD} == x ]; then
          echo "Set the ELASTIC_PASSWORD environment variable in the .env file";
          exit 1;
        elif [ x${KIBANA_PASSWORD} == x ]; then
          echo "Set the KIBANA_PASSWORD environment variable in the .env file";
          exit 1;
        fi;
        if [ ! -f certs/ca.zip ]; then
          echo "Creating CA";
          bin/elasticsearch-certutil ca --silent --pem -out config/certs/ca.zip;
          unzip config/certs/ca.zip -d config/certs;
        fi;
        if [ ! -f certs/certs.zip ]; then
          echo "Creating certs";
          echo -ne \
          "instances:\n"\
          "  - name: es01\n"\
          "    dns:\n"\
          "      - es01\n"\
          "      - localhost\n"\
          "    ip:\n"\
          "      - 127.0.0.1\n"\
          > config/certs/instances.yml;
          bin/elasticsearch-certutil cert --silent --pem -out config/certs/certs.zip --in config/certs/instances.yml --ca-cert config/certs/ca/ca.crt --ca-key config/certs/ca/ca.key;
          unzip config/certs/certs.zip -d config/certs;
        fi;
        echo "Setting file permissions"
        chown -R 9999:9999 config/certs # refers to user _flink_ from official flink image
        find . -type d -exec chmod 755 \{\} \;;
        find . -type f -exec chmod 644 \{\} \;;
        echo "Waiting for Elasticsearch availability";
        until curl -s --cacert config/certs/ca/ca.crt ${ELASTICSEARCH_URL} | grep -q "missing authentication credentials"; do sleep 30; done;
        echo "Setting kibana_system password";
        until curl -s -X POST --cacert config/certs/ca/ca.crt -u ${ELASTIC_USERNAME}:${ELASTIC_PASSWORD} -H "Content-Type: application/json" ${ELASTICSEARCH_URL}/_security/user/kibana_system/_password -d "{\"password\":\"${KIBANA_PASSWORD}\"}" | grep -q "^{}"; do sleep 10; done;
        echo "All done!";
      '
    healthcheck:
      test: ["CMD-SHELL", "[ -f config/certs/es01/es01.crt ]"]
      interval: 1s
      timeout: 5s
      retries: 120

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:${ELASTIC_STACK_VERSION}
    depends_on:
      setup:
        condition: service_healthy
    volumes:
      - certs:/usr/share/elasticsearch/config/certs
      - es-data:/usr/share/elasticsearch/data
    env_file: ../services/elastic/elasticsearch.env
    network_mode: service:dev
    restart: unless-stopped
    ulimits:
      memlock:
        soft: -1
        hard: -1
    healthcheck:
      test: ['CMD-SHELL', "curl -s --cacert config/certs/ca/ca.crt ${ELASTICSEARCH_URL} | grep -q 'missing authentication credentials'"]
      interval: 10s
      timeout: 10s
      retries: 120

  kibana:
    depends_on:
      elasticsearch:
        condition: service_healthy
      setup:
        condition: service_completed_successfully
    image: docker.elastic.co/kibana/kibana:${ELASTIC_STACK_VERSION}
    volumes:
      - certs:/usr/share/kibana/config/certs
      - kibana-data:/usr/share/kibana/data
    env_file: ../services/elastic/kibana.env
    environment:
      SERVERNAME: kibana
    network_mode: service:dev
    restart: unless-stopped
    healthcheck:
      test:
        [
          'CMD-SHELL',
          "curl -s -I ${KIBANA_URL} | grep -q 'HTTP/1.1 302 Found'",
        ]
      interval: 10s
      timeout: 10s
      retries: 120

  enterprisesearch:
    depends_on:
      kibana:
        condition: service_healthy
    image: docker.elastic.co/enterprise-search/enterprise-search:${ELASTIC_STACK_VERSION}
    volumes:
      - enterprisesearchdata:/usr/share/enterprise-search/config
      - certs:/usr/share/enterprise-search/config/certs
    env_file: ../services/elastic/enterprisesearch.env
    environment:
      SERVERNAME: enterprisesearch
    network_mode: service:dev
    healthcheck:
      test:
        [
            "CMD-SHELL",
            "curl -s -I ${ENTERPRISE_SEARCH_URL} | grep -q 'HTTP/1.1 302 Found'",
        ]
      interval: 10s
      timeout: 10s
      retries: 120

configs:
  atlas-application:
    file: ./apache-atlas/atlas-application.properties
  atlas-simple-authz-policy:
    file: ./apache-atlas/atlas-simple-authz-policy.json
  atlas-keycloak-conf:
    file: ./apache-atlas/keycloak-conf.json
  keycloak-config:
    file: ./keycloak/settings/standalone.xml
  keycloak-realm:
    file: ./keycloak/realms/atlas-dev.json

volumes:
  atlas-data:
  certs:
  checkpoints:
  es-data:
  kafka-data:
  keycloak-data:
  kibana-data:
  enterprisesearchdata:
