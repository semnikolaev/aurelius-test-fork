version: '3'

services:
  dev:
    build: .
    environment:
      FLINK_PROPERTIES: |
        jobmanager.rpc.address: localhost
        taskmanager.numberOfTaskSlots: 3
      ATLAS_SERVER_URL: "http://127.0.0.1:21000/api/atlas"
      ATLAS_USERNAME: "atlas"
      ELASTICSEARCH_APP_SEARCH_INDEX_NAME: ".ent-search-engine-documents-atlas-dev"
      ELASTICSEARCH_STATE_INDEX_NAME: "publish_state"
      ELASTICSEARCH_USERNAME: ${ELASTIC_USERNAME}
      ELASTICSEARCH_PASSWORD: ${ELASTIC_PASSWORD}
      ELASTICSEARCH_ENDPOINT: "https://localhost:9200"
      ELASTIC_URL: 'https://localhost:9200'
      ELASTIC_PASSWORD: 'elasticpw'
      ENTERPRISE_SEARCH_URL: 'http://localhost:3002/'
      DATA2MODEL_URL: 'http://localhost:7000'
      KAFKA_APP_SEARCH_TOPIC_NAME: ".ent-search-engine-documents-atlas-dev"
      KAFKA_PUBLISH_STATE_TOPIC_NAME: "publish_state"
      KAFKA_GOV_DATA_QUALITY_TOPIC_NAME: ".ent-search-engine-documents-atlas-dev-gov-quality"
      KAFKA_BOOTSTRAP_SERVER_HOSTNAME: "localhost"
      KAFKA_BOOTSTRAP_SERVER_PORT: "9092"
      KAFKA_CONSUMER_GROUP_ID: "SYNCHRONIZE_APP_SEARCH_CONSUMER"
      KAFKA_PRODUCER_GROUP_ID: "SYNCHRONIZE_APP_SEARCH_PRODUCER"
      KAFKA_ERROR_TOPIC_NAME: "DEAD_LETTER_BOX"
      KAFKA_SOURCE_TOPIC_NAME: "ATLAS_ENTITIES"
      KEYCLOAK_CLIENT_ID: "m4i_atlas"
      KEYCLOAK_PASSWORD: "atlas"
      KEYCLOAK_ATLAS_ADMIN_PASSWORD: "atlas"
      KEYCLOAK_REALM_NAME: "atlas-dev"
      KEYCLOAK_SERVER_URL: "http://127.0.0.1:8180/auth/"
      KEYCLOAK_USERNAME: "atlas"
      KEYCLOAK_ATLAS_USER_USERNAME: 'atlas'
      KEYCLOAK_ATLAS_USER_PASSWORD: 'atlas'
      ENTERPRISE_SEARCH_EXTERNAL_URL: 'http://localhost:3200'
      ATLAS_EXTERNAL_URL: 'http://localhost:8087/atlas/atlas'
      ELASTIC_CERTIFICATE_PATH: '/opt/flink/certs/ca/ca.crt'
      ELASTICSEARCH_CERTIFICATE_PATH: '/opt/flink/certs/ca/ca.crt'
      UPLOAD_DATA: "false"
      DATA_DICTIONARY_PATH: "/workspace/apps/m4i-data-dictionary-io/m4i_data_dictionary_io/example/Data Dictionary in IoT Terms.xlsm"
    command: 'taskmanager --cap-add=SYS_PTRACE --security-opt seccomp=unconfined'
    cap_add:
      - SYS_PTRACE
    volumes:
      - ..:/workspace
      - certs:/opt/flink/certs
    ports:
      - "127.0.0.1:5601:5601"
      - "127.0.0.1:8081:8081"
      - "127.0.0.1:8082:8082"
      - "127.0.0.1:8180:8180"
      - "127.0.0.1:9200:9200"
      - "127.0.0.1:3002:3002"
      - "127.0.0.1:21000:21000"
    working_dir: /workspace

  atlas:
    image: aureliusatlas/docker-apache-atlas:2.2.1
    depends_on:
      - kafka-broker
    volumes:
      - atlas-data:/opt/apache-atlas-2.2.0/data
    network_mode: service:dev
    restart: unless-stopped
    configs:
      - source: atlas-application
        target: /opt/apache-atlas-2.2.0/conf/atlas-application.properties
      - source: atlas-simple-authz-policy
        target: /opt/apache-atlas-2.2.0/conf/atlas-simple-authz-policy.json
      - source: atlas-keycloak-conf
        target: /opt/apache-atlas-2.2.0/conf/keycloak-conf.json
    command: "/opt/apache-atlas-2.2.0/bin/startup.sh"
    healthcheck:
      test:
        [
            "CMD-SHELL",
            "wget http://localhost:21000",
        ]
      interval: 10s
      timeout: 5s
      retries: 120

  jobmanager:
    image: flink:1.17.0
    depends_on:
      atlas:
        condition: service_healthy
    environment:
      FLINK_PROPERTIES: |
        jobmanager.rpc.address: localhost
        taskmanager.numberOfTaskSlots: 2
    command: jobmanager
    volumes:
      - checkpoints:/tmp/flink-checkpoints-directory
    network_mode: service:dev
    restart: unless-stopped

  kafka-broker:
    image: confluentinc/cp-kafka:7.6.1
    environment:
      CLUSTER_ID: 'ZDhiOWIzN2FmNmE0NWNlNW'
      KAFKA_ADVERTISED_LISTENERS: 'INTERNAL://localhost:9092'
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true'
      KAFKA_BROKER_ID: 1
      KAFKA_CONTROLLER_LISTENER_NAMES: 'CONTROLLER'
      KAFKA_CONTROLLER_QUORUM_VOTERS: '1@localhost:9093'
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_INTER_BROKER_LISTENER_NAME: 'INTERNAL'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: 'CONTROLLER:PLAINTEXT,INTERNAL:PLAINTEXT'
      KAFKA_LISTENERS: 'INTERNAL://localhost:9092,CONTROLLER://localhost:9093'
      KAFKA_NODE_ID: 1
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_PROCESS_ROLES: 'broker,controller'
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
    volumes:
      - kafka-data:/var/lib/kafka/data
    network_mode: service:dev
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "nc", "-z", "localhost", "9092"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s

  kafka-connect:
    build:
      context: ..
      dockerfile: ./services/kafka-connect/Dockerfile
      args:
        CONNECT_ELASTICSEARCH_VERSION: '14.0.11'
        CONNECT_VERSION: '7.3.2'
    depends_on:
      kafka-broker:
        condition: service_healthy
      setup:
        condition: service_completed_successfully
    volumes:
      - certs:/home/appuser/secret_certs
    environment:
      CONNECT_BOOTSTRAP_SERVERS: 'localhost:9092'
      CONNECT_CONFIG_STORAGE_CLEANUP_POLICY: 'compact'
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_CONFIG_STORAGE_TOPIC: 'kafka-connect-configs'
      CONNECT_GROUP_ID: 'kafka-connect-group'
      CONNECT_KEY_CONVERTER: 'org.apache.kafka.connect.storage.StringConverter'
      CONNECT_KEY_CONVERTER_SCHEMAS_ENABLE: 'false'
      CONNECT_LOG4J_ROOT_LOGLEVEL: 'INFO'
      CONNECT_OFFSET_STORAGE_CLEANUP_POLICY: 'compact'
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_OFFSET_STORAGE_TOPIC: 'kafka-connect-offsets'
      CONNECT_REST_ADVERTISED_HOST_NAME: 'localhost'
      CONNECT_REST_PORT: 8083
      CONNECT_STATUS_STORAGE_CLEANUP_POLICY: 'compact'
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_STATUS_STORAGE_TOPIC: 'kafka-connect-status'
      CONNECT_VALUE_CONVERTER: 'org.apache.kafka.connect.json.JsonConverter'
      CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: 'false'
      SECRET_FILE_PATH: /home/appuser/certs/ca/ca.crt
    network_mode: service:dev

  kafka-ui:
    image: provectuslabs/kafka-ui:v0.7.2
    depends_on:
      - kafka-broker
    environment:
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: 'localhost:9092'
      KAFKA_CLUSTERS_0_NAME: 'local'
      SERVER_PORT: 8082
    network_mode: service:dev
    restart: unless-stopped

  keycloak:
    image: quay.io/keycloak/keycloak:16.1.1
    environment:
      KEYCLOAK_IMPORT: '/tmp/keycloak-config/atlas-dev.json'
      KEYCLOAK_PASSWORD: 'admin'
      KEYCLOAK_PORT: 8180
      KEYCLOAK_REALM_NAME: 'atlas-dev'
      KEYCLOAK_USER: 'admin'
    volumes:
      - keycloak-data:/opt/jboss/keycloak/standalone/data
    configs:
      - source: keycloak-config
        target: /opt/jboss/keycloak/standalone/configuration/standalone.xml
      - source: keycloak-realm
        target: /tmp/keycloak-config/atlas-dev.json
    network_mode: service:dev
    restart: unless-stopped
    entrypoint: ["/bin/sh", "-c"]
    command: >
      '
      /opt/jboss/tools/docker-entrypoint.sh -b 0.0.0.0 -bmanagement 0.0.0.0 -Djboss.http.port=8180 2>&1 &
      while [[ "$(curl -s -o /dev/null -w ''%{http_code}'' curl -X GET http://localhost:8180/ | tail -c 3)" != "200" ]]; do
          echo "Waiting for Keycloak to start on port 8180...";
          sleep 5;
      done;
      sleep 5;
      /opt/jboss/keycloak/bin/kcadm.sh config credentials --server "http://localhost:8180/auth" --realm master --user admin --password admin;
      /opt/jboss/keycloak/bin/kcadm.sh create users -r atlas-dev -s username=atlas -s enabled=true;
      /opt/jboss/keycloak/bin/kcadm.sh add-roles -r atlas-dev --uusername atlas --rolename ROLE_ADMIN;
      /opt/jboss/keycloak/bin/kcadm.sh add-roles -r atlas-dev --uusername atlas --rolename DATA_STEWARD;
      /opt/jboss/keycloak/bin/kcadm.sh set-password -r atlas-dev --username atlas --new-password atlas;
      echo "atlas user created.";
      tail -f /dev/null
      '

  #################
  # ELASTIC STACK #
  #################
  setup:
    image: docker.elastic.co/elasticsearch/elasticsearch:${ELASTIC_STACK_VERSION}
    user: "0"
    network_mode: service:dev
    volumes:
      - certs:/usr/share/elasticsearch/config/certs
    command: >
      bash -c '
        if [ x${ELASTIC_PASSWORD} == x ]; then
          echo "Set the ELASTIC_PASSWORD environment variable in the .env file";
          exit 1;
        elif [ x${KIBANA_PASSWORD} == x ]; then
          echo "Set the KIBANA_PASSWORD environment variable in the .env file";
          exit 1;
        fi;
        if [ ! -f certs/ca.zip ]; then
          echo "Creating CA";
          bin/elasticsearch-certutil ca --silent --pem -out config/certs/ca.zip;
          unzip config/certs/ca.zip -d config/certs;
        fi;
        if [ ! -f certs/certs.zip ]; then
          echo "Creating certs";
          echo -ne \
          "instances:\n"\
          "  - name: es01\n"\
          "    dns:\n"\
          "      - es01\n"\
          "      - localhost\n"\
          "    ip:\n"\
          "      - 127.0.0.1\n"\
          > config/certs/instances.yml;
          bin/elasticsearch-certutil cert --silent --pem -out config/certs/certs.zip --in config/certs/instances.yml --ca-cert config/certs/ca/ca.crt --ca-key config/certs/ca/ca.key;
          unzip config/certs/certs.zip -d config/certs;
        fi;
        echo "Setting file permissions"
        chown -R 9999:9999 config/certs # refers to user _flink_ from official flink image
        find . -type d -exec chmod 755 \{\} \;;
        find . -type f -exec chmod 644 \{\} \;;
        echo "Waiting for Elasticsearch availability";
        until curl -s --cacert config/certs/ca/ca.crt ${ELASTICSEARCH_URL} | grep -q "missing authentication credentials"; do sleep 30; done;
        echo "Setting kibana_system password";
        until curl -s -X POST --cacert config/certs/ca/ca.crt -u ${ELASTIC_USERNAME}:${ELASTIC_PASSWORD} -H "Content-Type: application/json" ${ELASTICSEARCH_URL}/_security/user/kibana_system/_password -d "{\"password\":\"${KIBANA_PASSWORD}\"}" | grep -q "^{}"; do sleep 10; done;
        echo "All done!";
      '
    healthcheck:
      test: ["CMD-SHELL", "[ -f config/certs/es01/es01.crt ]"]
      interval: 1s
      timeout: 5s
      retries: 120

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:${ELASTIC_STACK_VERSION}
    depends_on:
      setup:
        condition: service_healthy
    volumes:
      - certs:/usr/share/elasticsearch/config/certs
      - es-data:/usr/share/elasticsearch/data
    env_file: ../services/elastic/elasticsearch.env
    network_mode: service:dev
    restart: unless-stopped
    ulimits:
      memlock:
        soft: -1
        hard: -1
    healthcheck:
      test: ['CMD-SHELL', "curl -s --cacert config/certs/ca/ca.crt ${ELASTICSEARCH_URL} | grep -q 'missing authentication credentials'"]
      interval: 10s
      timeout: 10s
      retries: 120

  kibana:
    depends_on:
      elasticsearch:
        condition: service_healthy
      setup:
        condition: service_completed_successfully
    image: docker.elastic.co/kibana/kibana:${ELASTIC_STACK_VERSION}
    volumes:
      - certs:/usr/share/kibana/config/certs
      - kibana-data:/usr/share/kibana/data
    env_file: ../services/elastic/kibana.env
    environment:
      SERVERNAME: kibana
    network_mode: service:dev
    restart: unless-stopped
    healthcheck:
      test:
        [
          'CMD-SHELL',
          "curl -s -I ${KIBANA_URL} | grep -q 'HTTP/1.1 302 Found'",
        ]
      interval: 10s
      timeout: 10s
      retries: 120

  enterprisesearch:
    depends_on:
      kibana:
        condition: service_healthy
    image: docker.elastic.co/enterprise-search/enterprise-search:${ELASTIC_STACK_VERSION}
    volumes:
      - enterprisesearchdata:/usr/share/enterprise-search/config
      - certs:/usr/share/enterprise-search/config/certs
    env_file: ../services/elastic/enterprisesearch.env
    environment:
      SERVERNAME: enterprisesearch
    network_mode: service:dev
    healthcheck:
      test:
        [
            "CMD-SHELL",
            "curl -s -I ${ENTERPRISE_SEARCH_URL} | grep -q 'HTTP/1.1 302 Found'",
        ]
      interval: 10s
      timeout: 10s
      retries: 120

  #post-install:
  #  build:
  #    context: ..
  #    dockerfile: ./backend/m4i-atlas-post-install/Dockerfile
  #  container_name: post-install
  #  env_file: ../backend/m4i-atlas-post-install/.env
  #  volumes:
  #    - certs:/opt/flink/certs
  #  restart: "no"
  #  depends_on:
  #    - jobmanager
  #  entrypoint: [ "bash", "-c", "./run.sh && tail -f /dev/null"]
  #  network_mode: service:dev

configs:
  atlas-application:
    file: ./apache-atlas/atlas-application.properties
  atlas-simple-authz-policy:
    file: ./apache-atlas/atlas-simple-authz-policy.json
  atlas-keycloak-conf:
    file: ./apache-atlas/keycloak-conf.json
  keycloak-config:
    file: ./keycloak/settings/standalone.xml
  keycloak-realm:
    file: ./keycloak/realms/atlas-dev.json

volumes:
  atlas-data:
  certs:
  checkpoints:
  es-data:
  kafka-data:
  keycloak-data:
  kibana-data:
  enterprisesearchdata:
